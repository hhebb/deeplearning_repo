{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import zip_longest\n",
    "import io\n",
    "from einops import rearrange, reduce, repeat\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, 3, padding='same'),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "\n",
    "            torch.nn.Conv2d(8, 16, 3, padding='same'),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(16, 8, 2, stride=2),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.ConvTranspose2d(8, 1, 2, stride=2),\n",
    "            torch.nn.BatchNorm2d(1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "    def predict(self, x):\n",
    "        pred = self.encoder(x)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        mnist = MNIST(root='/home/hebb/ml/datasets/', download=True, train=True)\n",
    "        self.data = mnist.train_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx] / 255\n",
    "        image = image.reshape(shape=[1, 28, 28])\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='mean')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "\n",
    "# ds = Dataset()\n",
    "# loader = torch.utils.data.DataLoader(ds, batch_size=128)\n",
    "# # criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hebb/ml/tasks/auto_encoder/train.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# data = data.cuda()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     recon_batch, mu, log_var \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(recon_batch, data, mu, log_var)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/hebb/ml/tasks/auto_encoder/train.ipynb Cell 7\u001b[0m in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     mu, log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m784\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling(mu, log_var)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z), mu, log_var\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = model(data)\n",
    "        loss = criterion(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(64, 2).cuda()\n",
    "    sample = model.decoder(z).cuda()\n",
    "    \n",
    "    save_image(sample.view(64, 1, 28, 28), './samples/sample_' + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VAE' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hebb/ml/tasks/auto_encoder/train.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     latent \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m10\u001b[39m, \u001b[39m20\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     recon \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecode(latent[\u001b[39m6\u001b[39m])\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mreshape(\u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/hebb/ml/tasks/auto_encoder/train.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     plt\u001b[39m.\u001b[39mimshow(recon, cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    latent = torch.randn(10, 20)\n",
    "    recon = model.decode(latent[6]).squeeze().reshape(28, 28)\n",
    "    plt.imshow(recon, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dd979c77fcde639c21f4338339a22090b916d4a09dd85e8db78c65a8dce030e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
