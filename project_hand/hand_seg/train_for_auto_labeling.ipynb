{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data dir\n",
    "# dataset, dataloader\n",
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HEBB\\anaconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhhebb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>\\\\wsl.localhost\\Ubuntu-20.04\\home\\hebb\\ml\\project_hand\\hand_seg\\wandb\\run-20221218_034919-1qhttb7a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hhebb/ml-project_hand_hand_seg/runs/1qhttb7a\" target=\"_blank\">skilled-butterfly-126</a></strong> to <a href=\"https://wandb.ai/hhebb/ml-project_hand_hand_seg\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/hhebb/ml-project_hand_hand_seg/runs/1qhttb7a?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1f76a773880>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from utils.util_auto_labeling import *\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import numpy as np\n",
    "from models.fcn import FCN\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetClean(Dataset):\n",
    "    def __init__(self, transform):\n",
    "        self.df = pd.read_csv(r'\\\\wsl.localhost\\Ubuntu-20.04\\home\\hebb\\ml\\datasets\\hand_youtube\\lut.csv')\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_path = self.df.iloc[idx]['image'], self.df.iloc[idx]['label']\n",
    "        img = np.array(Image.open(img_path)).astype(np.float32)\n",
    "        mask = json_to_array(label_path) / 255 > .5\n",
    "        mask = mask.astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=img, mask=mask)\n",
    "            img = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(),\n",
    "        A.Resize(480, 640),\n",
    "        # A.RandomCrop(320, 320),\n",
    "        A.ColorJitter(brightness=.1, contrast=.1, saturation=.1, hue=.1, p=.2),\n",
    "        A.Affine(translate_percent=.2),\n",
    "        A.Rotate(limit=30),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_valid = A.Compose(\n",
    "    [\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds = DatasetClean(transform)\n",
    "train_set, val_set = torch.utils.data.random_split(ds, [int(len(ds)*.95), len(ds)-int(len(ds)*.95)])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = DataLoader(val_set, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, mask = ds.__getitem__(0)\n",
    "# print(img.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "# plt.imshow(mask[..., 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_dice(pred, gt):\n",
    "    losses = torch.stack([BCEDice(pred[:, i, ...], \\\n",
    "        gt[:, i, ...]) for i in range(pred.shape[1])])\n",
    "    loss = torch.sum(losses)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def BCEDice(pred, gt):\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    bce = criterion(pred, gt)\n",
    "    dice = 1 - get_dice(pred, gt)\n",
    "    loss = bce + dice\n",
    "\n",
    "    return loss\n",
    "\n",
    "def get_dice(pred, gt):\n",
    "    eps = 1e-5\n",
    "    summ = torch.sum(gt) + torch.sum(pred)\n",
    "    inter = torch.sum(gt * pred)\n",
    "    dice = 2 * inter / (summ + eps)\n",
    "    \n",
    "    return dice\n",
    "\n",
    "def get_dice_metric(pred, gt):\n",
    "    eps = 1e-5\n",
    "    pred = pred > .5\n",
    "    \n",
    "    dices = list()\n",
    "    for ch in range(pred.shape[1]):\n",
    "        p = pred[:, ch, ...]\n",
    "        g = gt[:, ch, ...]\n",
    "        summ = torch.sum(p) + torch.sum(g)\n",
    "        inter = torch.sum(p * g)\n",
    "        dice = 2 * inter / (summ + eps)\n",
    "        dices.append(dice.cpu().numpy())\n",
    "\n",
    "    # eps = 1e-5\n",
    "    # pred = pred > .5\n",
    "    # summ = torch.sum(gt) + torch.sum(pred)\n",
    "    # inter = torch.sum(gt * pred)\n",
    "    # dice = 2 * inter / (summ + eps)\n",
    "    \n",
    "    return dices\n",
    "    \n",
    "model = FCN(classes=2).cuda()\n",
    "# model = UNet(n_channels=3, n_classes=3).cuda()\n",
    "# wandb.watch(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save 0\n",
      "save 1\n",
      "save 4\n",
      "save 5\n",
      "save 10\n",
      "save 13\n",
      "save 14\n",
      "save 26\n",
      "save 32\n",
      "save 50\n",
      "save 71\n",
      "save 75\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "model.train()\n",
    "max_dice = 0\n",
    "\n",
    "for e, epoch in enumerate(range(100)):\n",
    "    total_loss = 0\n",
    "    start = time.time()\n",
    "    for imgs, masks in train_loader:\n",
    "        imgs, masks = imgs.cuda(), masks[..., :-1].cuda()\n",
    "        pred = model(imgs)\n",
    "        masks = masks.permute(0, 3, 1, 2)\n",
    "\n",
    "        loss = total_dice(pred, masks)\n",
    "        \n",
    "        total_loss += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging batch\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        dices = list()\n",
    "        # valid\n",
    "        for batch_idx, (imgs, masks) in enumerate(valid_loader):\n",
    "            imgs, masks = imgs.cuda(), masks.cuda()\n",
    "            masks = masks.permute(0, 3, 1, 2)\n",
    "\n",
    "            pred = model(imgs)\n",
    "            dices.append(get_dice_metric(pred, masks))\n",
    "\n",
    "            # test output save\n",
    "            if e % 5 == 0:\n",
    "                for sample_idx, pre in enumerate(pred):\n",
    "                    im = imgs[sample_idx]\n",
    "                    im = im.cpu().numpy().transpose(1, 2, 0)\n",
    "                    im = (im * (0.229, 0.224, 0.225) + (0.485, 0.456, 0.406)) * 255\n",
    "                    im = im.astype(np.uint8)\n",
    "                    # pre = ((pre.squeeze().detach().cpu().numpy()).transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "                    pre = ((pre.squeeze().detach().cpu().numpy()) * 255).astype(np.uint8)\n",
    "\n",
    "                    im = np.clip(im + np.transpose(pre[0:1, ...], (1, 2, 0)) + np.transpose(pre[1:2, ...], (1, 2, 0)), 0, 255)\n",
    "                    # im = np.clip(im + np.transpose(np.stack([pre for i in range(3)]), (1, 2, 0)), 0, 255)\n",
    "                    # im = np.clip(im + np.transpose(np.stack([pre[..., 1] for i in range(3)]), (1, 2, 0)), 0, 255)\n",
    "                    im = Image.fromarray(im)\n",
    "                    save_path = os.path.join(r'\\\\wsl.localhost\\Ubuntu-20.04\\home\\hebb\\ml\\project_hand\\hand_seg\\output\\test', f\"{batch_idx}_{sample_idx}.jpg\")\n",
    "                    im.save(save_path)\n",
    "\n",
    "\n",
    "        dices = np.sum(np.array(dices), axis=0)\n",
    "        wandb.log(\n",
    "            {\n",
    "                'dice_hand': dices[0] / len(valid_loader), \n",
    "                'dice_head': dices[1] / len(valid_loader), \n",
    "                # 'dice_background': dices[2] / len(valid_loader), \n",
    "                'loss': total_loss.item() / len(train_loader),\n",
    "                'elapse': time.time() - start\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if max_dice < dices.mean():\n",
    "        torch.save(model.state_dict(), 'auto_label.pt')\n",
    "        max_dice = dices.mean()\n",
    "        print(f'save {e}')\n",
    "    # logging epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execute auto labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 나머지 데이터를 auto label 할 필요는 없음\n",
    "# 오토 라벨링 대상 이미지를 별도의 디렉토리로 빼놓기\n",
    "# pred labels 에 미리 적용할 원본 이미지 복사해놓기\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "eval_path = r'\\\\wsl.localhost\\Ubuntu-20.04\\home\\hebb\\ml\\datasets\\hand_youtube\\predicted_labels'\n",
    "\n",
    "model = FCN(classes=2).cuda()\n",
    "model.load_state_dict(torch.load('auto_label.pt'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for path in glob(f'{eval_path}\\*'):\n",
    "        if '.jpg' not in path:\n",
    "            continue\n",
    "        \n",
    "        img = np.array(Image.open(path))\n",
    "        transformed = torch.unsqueeze(transform_valid(image=img)['image'], 0).cuda()\n",
    "        # transformed = torch.unsqueeze(torch.tensor(img).permute(2, 0, 1), 0).type(torch.float).cuda()\n",
    "        pred = model(transformed)\n",
    "        pred = pred.squeeze().permute(1, 2, 0) > .5\n",
    "\n",
    "        # save original image, json\n",
    "        j = array_to_json(pred.detach().cpu().numpy(), path)\n",
    "        with open(path.replace('.jpg', '.json'), 'w') as f:\n",
    "            json.dump(j, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19a9830f5b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(pred.detach().cpu().numpy()[..., 1])\n",
    "plt.imshow(masks.cpu()[0, 1, ...])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "709cd8751d95dd4963018abf8a09bfd0af4b5f2a80d996782d6e3e302b255021"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
